{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We're going to use the `wikitext` (https://huggingface.co/datasets/wikitext) dataset with the `distilbert-base-cased` (https://huggingface.co/distilbert-base-cased) model checkpoint. \n","\n","Start by loading the `wikitext-2-raw-v1` version of that dataset, and take the 11th example (index 10) of the train split.\n","We'll tokenize this using the appropriate tokenizer, and we'll mask the sixth token (index 5) the sequence. \n","\n","When using the `distilbert-base-cased` checkpoint to unmask that (sixth token, index 5) token, what is the most probable predicted token (please provide the decoded token, and not the ID)? \n","\n","Tips: \n","- You might find the transformers docs (https://huggingface.co/docs/transformers/index) useful. \n","- You might find the datasets docs (https://huggingface.co/docs/datasets/index) useful. \n","- You might also be interested in the Hugging Face course (https://huggingface.co/course/chapter1/1)."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.8.0)\n","Requirement already satisfied: evaluate in /usr/local/python/3.10.4/lib/python3.10/site-packages (0.4.0)\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/python/3.10.4/lib/python3.10/site-packages (4.25.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (4.64.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (1.24.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (10.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.70.14)\n","Requirement already satisfied: xxhash in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (3.2.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (2023.1.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.11.1)\n","Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (1.5.2)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.3.6)\n","Requirement already satisfied: aiohttp in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (3.8.3)\n","Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (2.28.1)\n","Requirement already satisfied: filelock in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers[sentencepiece]) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.2)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.97)\n","Requirement already satisfied: protobuf<=3.20.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.20.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["! pip install datasets evaluate transformers[sentencepiece]\n","from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n","import datasets\n","import torch\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n","model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-cased\", output_attentions=True)\n","assert model.config.output_attentions == True\n","\n","dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","train_dataset = dataset[\"train\"]\n","\n","# Take the 11th example\n","example = train_dataset[10]\n","\n","# Tokenize the example\n","tokens = tokenizer.tokenize(example[\"text\"])\n","\n","# Mask the 6th token\n","tokens[5] = \"[MASK]\"\n","\n","# Convert the tokens to their IDs\n","input_ids = tokenizer.convert_tokens_to_ids(tokens)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'int' object has no attribute 'size'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Unmask the 6th token\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(model(input_ids[\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m      3\u001b[0m predicted_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(torch\u001b[39m.\u001b[39margmax(outputs[\u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m])\u001b[39m.\u001b[39mitem())\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe most probable predicted token is:\u001b[39m\u001b[39m\"\u001b[39m, predicted_token)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:663\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    661\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 663\u001b[0m dlbrt_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    664\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    665\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    666\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    667\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    668\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    669\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    670\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    671\u001b[0m )\n\u001b[1;32m    672\u001b[0m hidden_states \u001b[39m=\u001b[39m dlbrt_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    673\u001b[0m prediction_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_transform(hidden_states)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:563\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n","\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"]}],"source":["# Unmask the 6th token\n","outputs = model(input_ids[0]).size()\n","predicted_token = tokenizer.decode(torch.argmax(outputs[0, 5]).item())\n","\n","print(\"The most probable predicted token is:\", predicted_token)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.8.0)\n","Requirement already satisfied: evaluate in /usr/local/python/3.10.4/lib/python3.10/site-packages (0.4.0)\n","Requirement already satisfied: transformers in /usr/local/python/3.10.4/lib/python3.10/site-packages (4.25.1)\n","Requirement already satisfied: torch in /home/codespace/.local/lib/python3.10/site-packages (1.13.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (4.64.1)\n","Requirement already satisfied: aiohttp in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (3.8.3)\n","Requirement already satisfied: xxhash in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (3.2.0)\n","Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (1.5.2)\n","Requirement already satisfied: multiprocess in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.70.14)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (10.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (2023.1.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.11.1)\n","Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (2.28.1)\n","Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (1.24.0)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (6.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers) (3.9.0)\n","Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch) (4.4.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: wheel in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n","Requirement already satisfied: setuptools in /usr/local/python/3.10.4/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n","Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]},{"name":"stderr","output_type":"stream","text":["Found cached dataset wikitext (/home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n","100%|██████████| 3/3 [00:00<00:00, 33.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["<class 'list'>\n"]},{"ename":"AttributeError","evalue":"'list' object has no attribute 'size'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(input_ids))\n\u001b[1;32m     26\u001b[0m \u001b[39m# Unmask the 6th token\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m predicted_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(torch\u001b[39m.\u001b[39margmax(outputs[\u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m])\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe most probable predicted token is:\u001b[39m\u001b[39m\"\u001b[39m, predicted_token)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:663\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    661\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 663\u001b[0m dlbrt_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    664\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    665\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    666\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    667\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    668\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    669\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    670\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    671\u001b[0m )\n\u001b[1;32m    672\u001b[0m hidden_states \u001b[39m=\u001b[39m dlbrt_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    673\u001b[0m prediction_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_transform(hidden_states)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:563\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"]}],"source":["! pip install datasets evaluate transformers torch\n","from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n","import datasets\n","import torch\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n","model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n","\n","dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","train_dataset = dataset[\"train\"]\n","\n","# Take the 11th example\n","example = train_dataset[10]\n","\n","# Tokenize the example\n","tokens = tokenizer.tokenize(example[\"text\"])\n","\n","# Mask the 6th token\n","tokens[5] = \"[MASK]\"\n","\n","# Convert the tokens to their IDs\n","input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(type(input_ids))\n","\n","# Unmask the 6th token\n","outputs = model(input_ids)[0]\n","predicted_token = tokenizer.decode(torch.argmax(outputs[0, 5]).item())\n","\n","print(\"The most probable predicted token is:\", predicted_token)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.8.0)\n","Requirement already satisfied: evaluate in /usr/local/python/3.10.4/lib/python3.10/site-packages (0.4.0)\n","Requirement already satisfied: transformers in /usr/local/python/3.10.4/lib/python3.10/site-packages (4.25.1)\n","Requirement already satisfied: torch in /home/codespace/.local/lib/python3.10/site-packages (1.13.1)\n","Requirement already satisfied: hydra-core in /usr/local/python/3.10.4/lib/python3.10/site-packages (1.3.1)\n","Requirement already satisfied: omegaconf in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.3.0)\n","Requirement already satisfied: bitarray in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.6.2)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (6.0)\n","Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (1.5.2)\n","Requirement already satisfied: packaging in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (10.0.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (1.24.0)\n","Requirement already satisfied: multiprocess in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.70.14)\n","Requirement already satisfied: responses<0.19 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (2023.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (3.8.3)\n","Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from datasets) (2.28.1)\n","Requirement already satisfied: xxhash in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (3.2.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from datasets) (0.11.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/python/3.10.4/lib/python3.10/site-packages (from transformers) (3.9.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch) (4.4.0)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n","Requirement already satisfied: setuptools in /usr/local/python/3.10.4/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n","Requirement already satisfied: wheel in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/python/3.10.4/lib/python3.10/site-packages (from hydra-core) (4.9.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n","Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]},{"ename":"TypeError","evalue":"load() missing 1 required positional argument: 'model'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitarray\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mwikitext-2-raw-v1\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Select the 11th example (index 10)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m example \u001b[39m=\u001b[39m dataset[\u001b[39m10\u001b[39m]\n","\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'model'"]}],"source":["! pip install datasets evaluate transformers torch hydra-core omegaconf bitarray\n","from transformers import pipeline, AutoTokenizer, AutoModelWithLMHead\n","import torch \n","# import hydra-core \n","import omegaconf\n","import bitarray\n","\n","# Load the dataset\n","dataset = torch.hub.load('wikitext-2-raw-v1', split='train')\n","\n","# Select the 11th example (index 10)\n","example = dataset[10]\n","\n","# Tokenize the example\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n","tokens = tokenizer.tokenize(example)\n","\n","# Mask the sixth token (index 5)\n","tokens[5] = tokenizer.mask_token\n","\n","# Unmask the token using the distilbert-base-cased model\n","model = AutoModelWithLMHead.from_pretrained('distilbert-base-cased')\n","predictor = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n","result = predictor(tokens)\n","\n","# Print the most probable predicted token\n","print(result[0]['token'])\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4 (main, Dec 19 2022, 20:24:16) [GCC 9.4.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":2}
