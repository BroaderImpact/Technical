{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1><b>GSK NLP Analyst Technical Case Study</b><h1>\n",
    "\n",
    "<h3>Christine C. Amuzie<h3>\n",
    "Broader Impact Data Services\n",
    "\n",
    "<i>Completed: 2021.12.10</i>\n",
    "\n",
    "<i>Last Modified: 2021.12.10</i>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select a drug from the dataset. Imagine that a leader for that product has asked you the following question: “What aspects do patients like about using our product, and what do people dislike?”\n",
    "Use the data to write a report for this leader that will address their question."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# #!/Users/christineamuzie/opt/anaconda3/envs/p37env/bin/python\n",
    "# CLI: \n",
    "# $ conda activate /Users/christineamuzie/opt/anaconda3/envs/p37env\n",
    "# $ python --version\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import conda\n",
    "# Set up installer\n",
    "/Users/christineamuzie/opt/anaconda3/envs/p37env/bin/pip install "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sys.path.append(\"/Users/christineamuzie/opt/anaconda3/envs/p37env/bin/python\")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from subprocess import call\n",
    "call(\"/Users/christineamuzie/opt/anaconda3/envs/p37env/bin/python /Users/christineamuzie/Documents/GitHub/Technical/GSK/analysis_1.0.ipynb\", shell=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/christineamuzie/opt/anaconda3/bin/python\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import conda\n",
    "conda activate py37env\n",
    "# from conda import tensorflow\n",
    "# Set up installer\n",
    "# Users/christineamuzie/opt/anaconda3/condabin/conda install tensorflow"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-038bb6ec700c>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-038bb6ec700c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    conda activate py37env\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Checking versions\n",
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers\n",
    "\n",
    "# Loading relevant libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Setting visualization parameters \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "#Setting pytorch parameters\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.5\n",
      "IPython version      : 7.20.0\n",
      "\n",
      "numpy       : 1.19.2\n",
      "pandas      : 1.2.2\n",
      "torch       : 1.10.0\n",
      "transformers: 4.13.0\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b10841f85489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "df = pd.read_csv(\"case_study_data.txt\", sep=\"\\t\", header=0)\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "sns.countplot(df.drugName)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Review Counts by Drug');\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have more reviews for Phentermine than the other drugs, so we can proceed with the analysis of Phentermine reviews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Preprocessing with BERT\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "sample_txt = \"It's a new dawn, it's a new day, it's a new life for me. And I'm feeling good!\"\n",
    "# Tokenization\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "tokenizer.sep_token, tokenizer.sep_token_id # sentence ending separation\n",
    "tokenizer.cls_token, tokenizer.cls_token_id # beginning sentences\n",
    "tokenizer.pad_token, tokenizer.pad_token_id # padding\n",
    "tokenizer.unk_token, tokenizer.unk_token_id # unknown\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=100,\n",
    "  truncation=True, # explicitly truncate to max length\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  padding='max_length', # remove deprecated pad_to_max_length\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "encoding.keys()\n",
    "\n",
    "# dict_keys(['input_ids', 'attention_mask']) # dict_keys doesn't work for py3\n",
    "vocab = list(df.keys())\n",
    "print(vocab)\n",
    "\n",
    "# https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(encoding['input_ids'][0])) # token ids\n",
    "encoding['input_ids'][0]\n",
    "print(len(encoding['attention_mask'][0])) # attention mask\n",
    "encoding['attention_mask']\n",
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]) # special tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "token_lens = [] # token length of each review\n",
    "for txt in df.review:\n",
    "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "  token_lens.append(len(tokens))\n",
    "\n",
    "sns.distplot(token_lens) # plot distribution\n",
    "plt.xlim([0, 512]);\n",
    "plt.xlabel('Token count');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MAX_LEN = 300 # majority of reviews < 300 tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# determining max length of review\n",
    "import numpy as np\n",
    "measurer = np.vectorize(len)\n",
    "res1 = measurer(df.values.astype(str)).max(axis=0)\n",
    "print(res1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RxReviewDataset(Dataset): # creating PyTorch dataset\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train, df_test = train_test_split( # split data into training and test\n",
    "  df,\n",
    "  test_size=0.1,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size): # create data loaders\n",
    "  ds = RxReviewDataset(\n",
    "    reviews=df.review.to_numpy(),\n",
    "    targets=df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 16 # set batch size\n",
    "MAX_LEN = 300 # majority of reviews < 300 tokens\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SentimentClassifier(nn.Module): # BERT-based classifier\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device) # move to GPU"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize afinn sentiment analyzer\n",
    "from afinn import Afinn\n",
    "af = Afinn()\n",
    "\n",
    "# compute sentiment scores (polarity) and labels\n",
    "sentiment_scores = [af.score(article) for article in corpus]\n",
    "sentiment_category = ['positive' if score > 0 \n",
    "                          else 'negative' if score < 0 \n",
    "                              else 'neutral' \n",
    "                                  for score in sentiment_scores]\n",
    "    \n",
    "    \n",
    "# sentiment statistics per news category\n",
    "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores, sentiment_category]).T\n",
    "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
    "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
    "df.groupby(by=['news_category']).describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conda install pip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Broader Impact Data Services, LLC &copy; 2021"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "817763465c47354bf57b37a0180640c2b176d3440cae2bf466c21d25f23dda3b"
  },
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}