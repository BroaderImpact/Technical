{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We're going to use the `wikitext` (https://huggingface.co/datasets/wikitext) dataset with the `distilbert-base-cased` (https://huggingface.co/distilbert-base-cased) model checkpoint. \n","\n","Start by loading the `wikitext-2-raw-v1` version of that dataset, and take the 11th example (index 10) of the train split.\n","We'll tokenize this using the appropriate tokenizer, and we'll mask the sixth token (index 5) the sequence. \n","\n","When using the `distilbert-base-cased` checkpoint to unmask that (sixth token, index 5) token, what is the most probable predicted token (please provide the decoded token, and not the ID)? \n","\n","Tips: \n","- You might find the transformers docs (https://huggingface.co/docs/transformers/index) useful. \n","- You might find the datasets docs (https://huggingface.co/docs/datasets/index) useful. \n","- You might also be interested in the Hugging Face course (https://huggingface.co/course/chapter1/1)."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Found cached dataset wikitext (/home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n","100%|██████████| 3/3 [00:00<00:00, 11.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["The most probable predicted token is: m e c h a n i c\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# ! pip install datasets evaluate transformers[sentencepiece]\n","from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n","import torch\n","import datasets\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n","model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n","\n","dataset = datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","train_dataset = dataset[\"train\"]\n","\n","# Take the 11th example\n","example = train_dataset[10]\n","\n","# Tokenize the example\n","tokens = tokenizer.tokenize(example[\"text\"])\n","\n","# Mask the 6th token\n","tokens[5] = \"[MASK]\"\n","\n","# Convert the tokens to their IDs\n","input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","# Convert input_ids to a tensor and add a dimension\n","input_ids = torch.tensor(input_ids).unsqueeze(0)\n","\n","# Unmask the 6th token\n","outputs = model(input_ids)[0]\n","predicted_token = tokenizer.decode(torch.argmax(outputs[0, 5]).item())\n","\n","print(\"The most probable predicted token is:\", predicted_token)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4 (main, Dec 19 2022, 20:24:16) [GCC 9.4.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":2}
